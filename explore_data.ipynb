{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "from itertools import permutations\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import pickle\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore all datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## METR-LA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_hdf('data/METRLA/metr-la.h5')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_entries = df.size\n",
    "zero_entries = (df == 0).sum().sum()\n",
    "percentage_zeros = (zero_entries / total_entries) * 100\n",
    "percentage_zeros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PEMS-BAY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_hdf('data/PEMSBAY/PEMS-BAY.h5')\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_entries = df2.size\n",
    "zero_entries = (df2 == 0).sum().sum()\n",
    "percentage_zeros = (zero_entries / total_entries) * 100\n",
    "percentage_zeros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HAUGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = pd.read_hdf('data/Hauge/hague_comp_filled.h5', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_entries = df3.size\n",
    "zero_entries = (df3 == 0).sum().sum()\n",
    "percentage_zeros = (zero_entries / total_entries) * 100\n",
    "percentage_zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4 = pd.read_hdf('data/Hauge/hague_filled.h5', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Original data for one specific intersection. Legacy code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = df3.iloc[:-1]\n",
    "df3['K074'] = pd.to_datetime(df3['K074'])\n",
    "df3_first_day = df3[df3['K074'].dt.date == df3['K074'].dt.date.min()]\n",
    "\n",
    "# Plot the distribution of values in column '021' over time\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(df3_first_day['K074'], df3_first_day['021'], marker='o')\n",
    "plt.title('Distribution of Sensor 021 at Area K074 on 01/01/2012')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('021 Values')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()  # Adjust layout to prevent clipping of tick-labels\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4 = pd.read_csv('data/Hauge/K415/K415-2018-1-.csv', sep=';')\n",
    "df4 = df4.drop(df4.index[-1])\n",
    "df4.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check adjacency matrix\n",
    "### METR-LA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "file_path = 'data/METRLA/adj_mx.pkl'\n",
    "\n",
    "with open(file_path, 'rb') as f:\n",
    "    adj_mx = pickle.load(f, encoding='latin1')\n",
    "\n",
    "print(len(adj_mx))\n",
    "print(adj_mx[0])\n",
    "print(len(adj_mx[0]))\n",
    "print(adj_mx[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(adj_mx[2][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_mx[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate new distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test on two points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# osrm_server = \"http://router.project-osrm.org\"\n",
    "osrm_server = \"http://127.0.0.1:5001\"\n",
    "x1 = '4.229222222222222'\n",
    "y1 = '52.05197222222222'\n",
    "\n",
    "x2 = '4.228972222222223'\n",
    "y2 = '52.05175'\n",
    "\n",
    "\n",
    "request_url = f\"{osrm_server}/route/v1/driving/{x1},{y1};{x2},{y2}?overview=false\"\n",
    "print(request_url)\n",
    "# Making the request to the OSRM API\n",
    "response = requests.get(request_url)\n",
    "route_data = response.json()\n",
    "print(route_data)\n",
    "\n",
    "# Parsing the distance from the response\n",
    "# Note: Make sure to handle any errors or unexpected response formats in a real application\n",
    "if route_data['code'] == 'Ok':\n",
    "    cost = route_data['routes'][0]['distance']  # Distance in meters\n",
    "    print(f\"The distance between the two points is {cost} meters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Illustration of two points on map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from geopy.distance import geodesic\n",
    "import folium\n",
    "from IPython.display import display\n",
    "\n",
    "def compute_and_visualize_distances(sensor_a, sensor_b):\n",
    "    osrm_server = \"http://router.project-osrm.org\"\n",
    "\n",
    "    # Compute the direct distance using the Haversine formula\n",
    "    direct_distance = geodesic((sensor_a['latitude'], sensor_a['longitude']), (sensor_b['latitude'], sensor_b['longitude'])).meters\n",
    "    \n",
    "    # Compute the routing distance using the OSRM API\n",
    "    request_url = f\"{osrm_server}/route/v1/driving/{sensor_a['longitude']},{sensor_a['latitude']};{sensor_b['longitude']},{sensor_b['latitude']}?overview=full&geometries=geojson\"\n",
    "    response = requests.get(request_url)\n",
    "    route_data = response.json()\n",
    "    \n",
    "    if route_data['code'] == 'Ok':\n",
    "        routing_distance = route_data['routes'][0]['distance']  # Distance in meters\n",
    "        route_geometry = route_data['routes'][0]['geometry']\n",
    "    else:\n",
    "        routing_distance = float('inf')  # An arbitrary large number to denote failure to get distance\n",
    "        route_geometry = None\n",
    "    \n",
    "    # Create a map centered around the midpoint of the two sensors\n",
    "    midpoint = ((sensor_a['latitude'] + sensor_b['latitude']) / 2, (sensor_a['longitude'] + sensor_b['longitude']) / 2)\n",
    "    m = folium.Map(location=midpoint, zoom_start=13, tiles='CartoDB Positron')\n",
    "    \n",
    "    # Add markers for the two sensors\n",
    "    folium.Marker([sensor_a['latitude'], sensor_a['longitude']], popup=f\"Sensor A: {sensor_a['sensor_id']}\").add_to(m)\n",
    "    folium.Marker([sensor_b['latitude'], sensor_b['longitude']], popup=f\"Sensor B: {sensor_b['sensor_id']}\").add_to(m)\n",
    "    \n",
    "    # Draw a line for the direct distance\n",
    "    folium.PolyLine(\n",
    "        locations=[(sensor_a['latitude'], sensor_a['longitude']), (sensor_b['latitude'], sensor_b['longitude'])],\n",
    "        color='blue',\n",
    "        weight=2,\n",
    "        opacity=0.6,\n",
    "        tooltip=f\"Direct distance: {direct_distance:.2f} meters\"\n",
    "    ).add_to(m)\n",
    "    \n",
    "    # Draw a line for the routing distance if available\n",
    "    if route_geometry:\n",
    "        folium.PolyLine(\n",
    "            locations=[(coord[1], coord[0]) for coord in route_geometry['coordinates']],\n",
    "            color='green',\n",
    "            weight=2,\n",
    "            opacity=0.6,\n",
    "            tooltip=f\"Routing distance: {routing_distance:.2f} meters\"\n",
    "        ).add_to(m)\n",
    "    \n",
    "    # Display the map\n",
    "    display(m)\n",
    "\n",
    "    # Print distances\n",
    "    print(f\"Direct distance: {direct_distance:.2f} meters\")\n",
    "    print(f\"Routing distance: {routing_distance:.2f} meters\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensor_a = {'sensor_id': 9, 'latitude': 34.15562, 'longitude': -118.46860}  # Example coordinates for sensor A\n",
    "sensor_b = {'sensor_id': 75, 'latitude': 34.15571, 'longitude': -118.43273}  # Example coordinates for sensor B\n",
    "\n",
    "compute_and_visualize_distances(sensor_a, sensor_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hague"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_sensors_and_group_by_trajectory(csv_file_path):\n",
    "    trajectory_groups = {'T1N': [], 'T1S': [], 'T2N': [], 'T2S': []}\n",
    "    with open(csv_file_path, mode='r') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        for row in reader:\n",
    "            trajectories = row['trajectory'].split('+')\n",
    "            for trajectory in trajectories:\n",
    "                trajectory_groups[trajectory].append(row)\n",
    "    return trajectory_groups\n",
    "\n",
    "def calculate_distances_osrm_grouped(sensors_grouped):\n",
    "    # osrm_server = \"http://router.project-osrm.org\"\n",
    "    osrm_server = \"http://127.0.0.1:5001\"\n",
    "    results = []\n",
    "\n",
    "    for trajectory, sensors in sensors_grouped.items():\n",
    "        # Prepare the combinations and wrap it with tqdm for the progress bar\n",
    "        sensor_pairs = list(permutations(sensors, 2))\n",
    "        for sensor_a, sensor_b in tqdm(sensor_pairs, desc=f\"Calculating distances for {trajectory}\"):\n",
    "            request_url = f\"{osrm_server}/route/v1/driving/{sensor_a['longitude']},{sensor_a['latitude']};{sensor_b['longitude']},{sensor_b['latitude']}?overview=false\"\n",
    "            \n",
    "            response = requests.get(request_url)\n",
    "            route_data = response.json()\n",
    "            \n",
    "            if route_data['code'] == 'Ok':\n",
    "                cost = route_data['routes'][0]['distance']  # Distance in meters\n",
    "            else:\n",
    "                cost = float('inf')  # Indicate failure\n",
    "            \n",
    "            results.append({\n",
    "                'trajectory': trajectory,\n",
    "                'from': sensor_a['sensor_id'],\n",
    "                'to': sensor_b['sensor_id'],\n",
    "                'cost': cost\n",
    "            })\n",
    "            # print(f\"From {sensor_a['sensor_id']} to {sensor_b['sensor_id']} cost: {cost}\")\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file_path = 'data/Hauge/location_std.csv'\n",
    "sensors_grouped = read_sensors_and_group_by_trajectory(csv_file_path)\n",
    "results = calculate_distances_osrm_grouped(sensors_grouped)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save distances betwee each sensor on the same trajectory into a csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/Hauge/distances.csv', mode='w', newline='') as file:\n",
    "    writer = csv.DictWriter(file, fieldnames=['trajectory', 'from', 'to', 'cost'])\n",
    "    writer.writeheader()\n",
    "    writer.writerows(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## METR-LA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_sensors(csv_file_path):\n",
    "    sensors = []\n",
    "    with open(csv_file_path, mode='r') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        for row in reader:\n",
    "            sensors.append(row)\n",
    "    return sensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_distances_osrm(sensors):\n",
    "    results = []\n",
    "    osrm_server = \"http://router.project-osrm.org\"  # Example server, replace with your own if you have one\n",
    "    # osrm_server = \"http://127.0.0.1:5001\"\n",
    "    for sensor_a in tqdm(sensors, desc=\"Calculating distances\"):\n",
    "        for sensor_b in sensors:\n",
    "            if sensor_a['sensor_id'] == sensor_b['sensor_id']:\n",
    "                cost = 0.0  # Cost is zero when it's the same sensor\n",
    "            else:\n",
    "                # Constructing the request URL\n",
    "                request_url = f\"{osrm_server}/route/v1/driving/{sensor_a['longitude']},{sensor_a['latitude']};{sensor_b['longitude']},{sensor_b['latitude']}?overview=false\"\n",
    "                print(request_url)\n",
    "                # Making the request to the OSRM API\n",
    "                response = requests.get(request_url)\n",
    "                route_data = response.json()\n",
    "                \n",
    "                # Parsing the distance from the response\n",
    "                # Note: Make sure to handle any errors or unexpected response formats in a real application\n",
    "                if route_data['code'] == 'Ok':\n",
    "                    cost = route_data['routes'][0]['distance']  # Distance in meters\n",
    "                else:\n",
    "                    cost = float('inf')  # An arbitrary large number to denote failure to get distance\n",
    "                # print(cost)\n",
    "            results.append({'from': sensor_a['sensor_id'], 'to': sensor_b['sensor_id'], 'cost': cost})\n",
    "            # print(results)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file_path = 'data/METRLA/graph_sensor_locations.csv'\n",
    "output_csv_file_path = 'data/METRLA/distances_la_route.csv'\n",
    "\n",
    "sensors = read_sensors(csv_file_path)\n",
    "distance_results = calculate_distances_osrm(sensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(output_csv_file_path, mode='w', newline='') as csvfile:\n",
    "    fieldnames = ['from', 'to', 'cost']\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "\n",
    "    writer.writeheader()\n",
    "    for result in distance_results:\n",
    "        writer.writerow(result)\n",
    "\n",
    "print(f\"Distances saved to {output_csv_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PeMS-BAY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file_path = 'data/PEMSBAY/graph_sensor_locations_bay.csv'\n",
    "output_csv_file_path = 'data/PEMSBAY/distances_bay_route.csv'\n",
    "sensors = read_sensors(csv_file_path)\n",
    "distance_results = calculate_distances_osrm(sensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(output_csv_file_path, mode='w', newline='') as csvfile:\n",
    "    fieldnames = ['from', 'to', 'cost']\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "\n",
    "    writer.writeheader()\n",
    "    for result in distance_results:\n",
    "        writer.writerow(result)\n",
    "\n",
    "print(f\"Distances saved to {output_csv_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create new adjacency matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## METR-LA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distances_df = pd.read_csv('data/METRLA/distances_la_route.csv')\n",
    "df = pd.read_hdf('data/METRLA/metr-la.h5')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_adjacency_matrix(dist_mx_array, normalized_k=0.1):\n",
    "    # Calculate the standard deviation as theta for normalization\n",
    "    distances = dist_mx_array[~np.isinf(dist_mx_array)].flatten()\n",
    "    std = distances.std()\n",
    "    adj_mx_array = np.exp(-np.square(dist_mx_array / std))\n",
    "    \n",
    "    # Set entries below a threshold to zero for sparsity\n",
    "    adj_mx_array[adj_mx_array < normalized_k] = 0\n",
    "    return adj_mx_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensor_ids = df.columns.astype(str).tolist()\n",
    "\n",
    "# Efficiently convert sensor IDs to index for quick lookup (now all as strings)\n",
    "sensor_id_to_index = {str(sensor_id): index for index, sensor_id in enumerate(sensor_ids)}\n",
    "\n",
    "# Initialize an empty distance matrix\n",
    "num_sensors = len(sensor_ids)\n",
    "dist_mx_array = np.full((num_sensors, num_sensors), np.inf)\n",
    "\n",
    "# Make sure 'from' and 'to' in distances_df are also strings\n",
    "distances_df['from'] = distances_df['from'].astype(str)\n",
    "distances_df['to'] = distances_df['to'].astype(str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Populate the distance matrix with the data from distances_df\n",
    "for _, row in distances_df.iterrows():\n",
    "    from_sensor = row['from']\n",
    "    to_sensor = row['to']\n",
    "    if from_sensor in sensor_id_to_index and to_sensor in sensor_id_to_index:\n",
    "        i = sensor_id_to_index[from_sensor]\n",
    "        j = sensor_id_to_index[to_sensor]\n",
    "        dist_mx_array[i, j] = row['cost']\n",
    "\n",
    "# Replace the diagonal with zeros since the distance from a sensor to itself is zero\n",
    "np.fill_diagonal(dist_mx_array, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_mx_array = get_adjacency_matrix(dist_mx_array).astype(np.float32)\n",
    "adj_mx = [sensor_ids, sensor_id_to_index, adj_mx_array]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_mx[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = 'data/METRLA'\n",
    "file_name = 'adj_mx_new.pkl'\n",
    "file_path = os.path.join(directory, file_name)\n",
    "\n",
    "# Ensure the directory exists\n",
    "os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "# Save the adj_mx to a .pkl file\n",
    "with open(file_path, 'wb') as f:\n",
    "    pickle.dump(adj_mx, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PeMS-BAY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load distances from CSV\n",
    "distances_df = pd.read_csv('data/PEMSBAY/distances_bay_route.csv')\n",
    "df = pd.read_hdf('data/PEMSBAY/pems-bay.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_adjacency_matrix(dist_mx_array, normalized_k=0.1):\n",
    "    # Calculate the standard deviation as theta for normalization\n",
    "    distances = dist_mx_array[~np.isinf(dist_mx_array)].flatten()\n",
    "    std = distances.std()\n",
    "    adj_mx_array = np.exp(-np.square(dist_mx_array / std))\n",
    "    \n",
    "    # Set entries below a threshold to zero for sparsity\n",
    "    adj_mx_array[adj_mx_array < normalized_k] = 0\n",
    "    return adj_mx_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensor_ids = df.columns.astype(str).tolist()\n",
    "\n",
    "# Efficiently convert sensor IDs to index for quick lookup (now all as strings)\n",
    "sensor_id_to_index = {str(sensor_id): index for index, sensor_id in enumerate(sensor_ids)}\n",
    "\n",
    "# Initialize an empty distance matrix\n",
    "num_sensors = len(sensor_ids)\n",
    "dist_mx_array = np.full((num_sensors, num_sensors), np.inf)\n",
    "\n",
    "# Make sure 'from' and 'to' in distances_df are also strings\n",
    "distances_df['from'] = distances_df['from'].astype(str)\n",
    "distances_df['to'] = distances_df['to'].astype(str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Populate the distance matrix with the data from distances_df\n",
    "for _, row in distances_df.iterrows():\n",
    "    from_sensor = row['from']\n",
    "    to_sensor = row['to']\n",
    "    if from_sensor in sensor_id_to_index and to_sensor in sensor_id_to_index:\n",
    "        i = sensor_id_to_index[from_sensor]\n",
    "        j = sensor_id_to_index[to_sensor]\n",
    "        dist_mx_array[i, j] = row['cost']\n",
    "\n",
    "# Replace the diagonal with zeros since the distance from a sensor to itself is zero\n",
    "np.fill_diagonal(dist_mx_array, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_mx_array = get_adjacency_matrix(dist_mx_array).astype(np.float32)\n",
    "\n",
    "# Construct the adj_mx list as specified with the updated adj_mx_array\n",
    "adj_mx = [sensor_ids, sensor_id_to_index, adj_mx_array]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_mx[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = 'data/PEMSBAY'\n",
    "file_name = 'adj_mx_new.pkl'\n",
    "file_path = os.path.join(directory, file_name)\n",
    "\n",
    "# Ensure the directory exists\n",
    "os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "# Save the adj_mx to a .pkl file\n",
    "with open(file_path, 'wb') as f:\n",
    "    pickle.dump(adj_mx, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleansing Hague data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K159-2018-7-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv(\"data/Hauge/K159/K159-2018-7-1.csv\", delimiter=';')\n",
    "\n",
    "# Convert the first column to datetime, assuming the format \"day-month-year hours:minutes\"\n",
    "# Note: Adjust the 'dayfirst=True' parameter if your date format varies\n",
    "first_column_name = df.columns[0]\n",
    "df[first_column_name] = pd.to_datetime(df[first_column_name], dayfirst=True)\n",
    "\n",
    "# Format the date in the desired output format \"year-month-day hours:minutes\"\n",
    "df[first_column_name] = df[first_column_name].dt.strftime('%Y-%m-%d %H:%M')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"K159-2018-7-1_modified.csv\", sep=';', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K159-2018-7-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv(\"data/Hauge/K159/K159-2018-7-2.csv\", delimiter=';')\n",
    "df.loc[:len(df)-2, first_column_name] = pd.to_datetime(df[first_column_name].iloc[:-1], dayfirst=True).dt.strftime('%Y-%m-%d %H:%M')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"K159-2018-7-2_modified.csv\", sep=';', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K182-2019-9-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv(\"data/Hauge/K182/K182-2019-9-1.csv\", delimiter=';')\n",
    "\n",
    "# Convert the first column to datetime, assuming the format \"day-month-year hours:minutes\"\n",
    "# Note: Adjust the 'dayfirst=True' parameter if your date format varies\n",
    "first_column_name = df.columns[0]\n",
    "df[first_column_name] = pd.to_datetime(df[first_column_name], dayfirst=True)\n",
    "\n",
    "# Format the date in the desired output format \"year-month-day hours:minutes\"\n",
    "df[first_column_name] = df[first_column_name].dt.strftime('%Y-%m-%d %H:%M')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"K182-2019-9-1_modified.csv\", sep=';', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K182-2019-9-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/Hauge/K182/K182-2019-9-2.csv\", delimiter=';')\n",
    "df.loc[:len(df)-2, first_column_name] = pd.to_datetime(df[first_column_name].iloc[:-1], dayfirst=True).dt.strftime('%Y-%m-%d %H:%M')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"K182-2019-9-2_modified.csv\", sep=';', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create adjacency matrix HAUGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensors_dict = {\n",
    "        'K502': ['082', '051', '111', '081'],\n",
    "        'K504': ['08_2', '12_1', '02_1', '08_1', '11_1', '09_1', '12_2', '02_2', '04_2', '05_1', '04_1', '05_2'],\n",
    "        'K503': ['02_1', '03_1', '11_1', '07_1', '09_1', '05_1', '05_2', '11_2'],\n",
    "        'K263': ['052', '041', '061', '112', '051', '031', '111', '081'],\n",
    "        'K556': ['08_2', '02_1', '08_1', '11_1', '02_2', '10_1', '05_1', '04_1'],\n",
    "        'K557': ['08_2', '02_1', '08_1', '11_1', '02_2', '10_1'],\n",
    "        'K559': ['08_2', '12_1', '02_1', '08_1', '02_2', '10_1'],\n",
    "        'K561': ['08_2', '01_1', '02_1', '08_1', '09_1', '02_2', '10_1', '05_1', '05_2', '06_1'],\n",
    "        'K198': ['052', '061', '112', '021', '051', '111', '081'],\n",
    "        'K704': ['12_1', '02_1', '69_1', '11_1', '65_2', '11_3', '65_1', '11_2'],\n",
    "        'K702': ['03_1', '11_1', '05_1', '04_1', '05_2', '11_2'],\n",
    "        'K703': ['08_2', '02_1', '08_1', '03_1', '09_1', '05_1', '05_2'],\n",
    "        # 'K159': ['021', '051', '713', '111', '081'],\n",
    "        'K159': ['021', '051', '111', '081'],\n",
    "        'K182': ['621', '682', '681', '622', '121'],\n",
    "        'K183': ['082', '021', '051', '022', '111', '081'],\n",
    "        'K128': ['131', '161', '061', '112', '162', '062'],\n",
    "        # 'K139': ['101', '082', '021', '022', '121', '081'],\n",
    "        # 'K104': ['082', '021', '051', '022', '111', '081'],\n",
    "        'K101': ['621', '082', '051', '711', '622', '121', '081'],\n",
    "        'K206': ['091', '052', '101', '082', '061', '021', '051', '022', '092', '081'],\n",
    "        'K074': ['052', '051'],\n",
    "        'K414': ['091', '122', '082', '051', '011', '012', '121', '092', '081'],\n",
    "        'K415': ['061', '021', '081', '041'],\n",
    "        'K250': ['091', '101', '102', '021', '022', '092', '081']\n",
    "        }\n",
    "\n",
    "# Base path for the sensor data\n",
    "base_path = 'data/Hauge'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identify missing timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a complete DateTimeIndex from 2018-01-01 00:00:00 to 2019-12-31 23:55:00 with 5-minute intervals\n",
    "complete_index = pd.date_range(start='2018-01-01 00:00:00', end='2019-12-31 23:55:00', freq='5T')\n",
    "\n",
    "intersections = []\n",
    "missing_timestamps_dict = {}  # Initialize a dictionary to store missing timestamps for each sensor\n",
    "\n",
    "for sensor, codes in sensors_dict.items():\n",
    "    sensor_folder_path = os.path.join(base_path, sensor)\n",
    "    csv_files = glob.glob(f\"{sensor_folder_path}/{sensor}-201[89]-*-*.csv\")\n",
    "    all_data = []\n",
    "    for file in tqdm(csv_files, desc=f\"Processing {sensor}\"):\n",
    "        # Read the CSV file\n",
    "        df = pd.read_csv(file, delimiter=';', parse_dates=[0], index_col=0)\n",
    "        \n",
    "        # Clean the column names of quotes\n",
    "        df.columns = [col.replace('\"', '').replace(\"'\", \"\") for col in df.columns]\n",
    "        df = df[~df.index.astype(str).str.contains('totaal')]\n",
    "    \n",
    "        df.index.name = None\n",
    "        df.index = pd.to_datetime(df.index)\n",
    "        \n",
    "        # Keep only the filtered columns\n",
    "        df = df.loc[:, codes]\n",
    "        \n",
    "        # Append the DataFrame to the list\n",
    "        all_data.append(df)\n",
    "\n",
    "    # Concatenate all DataFrames into a single DataFrame\n",
    "    final_df = pd.concat(all_data)\n",
    "\n",
    "    # Sort the DataFrame by the index (date and time)\n",
    "    final_df.sort_index(inplace=True)\n",
    "    final_df.columns = [sensor + \"-\" + col for col in final_df.columns]\n",
    "    final_df = final_df[final_df.index.year.isin([2018, 2019])]\n",
    "    \n",
    "    # Identify missing timestamps\n",
    "    missing_timestamps = complete_index.difference(final_df.index)\n",
    "    \n",
    "    # Store the missing timestamps in the dictionary\n",
    "    missing_timestamps_dict[sensor] = missing_timestamps\n",
    "\n",
    "    # Print the first few rows to verify\n",
    "    # print(final_df.head())\n",
    "#     intersections.append(final_df)\n",
    "\n",
    "# hague = pd.concat(intersections, axis=1)\n",
    "# hague = hague.sort_index(axis=1)\n",
    "# missing_timestamps_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "save the missing timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Convert datetime objects to strings in missing_timestamps_dict\n",
    "for sensor, timestamps in missing_timestamps_dict.items():\n",
    "    missing_timestamps_dict[sensor] = [timestamp.isoformat() for timestamp in timestamps]\n",
    "\n",
    "# Save missing_timestamps_dict to a JSON file\n",
    "with open('missing_timestamps.json', 'w') as f:\n",
    "    json.dump(missing_timestamps_dict, f, indent=4)\n",
    "\n",
    "# The file missing_timestamps.json now contains the missing timestamps data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare and generate the standardized data file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intersections = []\n",
    "\n",
    "for sensor, codes in sensors_dict.items():\n",
    "    sensor_folder_path = os.path.join(base_path, sensor)\n",
    "    print(sensor_folder_path)\n",
    "\n",
    "    # List all CSV files in the sensor folder for the date range 2018-01 to 2019-12\n",
    "    csv_files = glob.glob(f\"{sensor_folder_path}/{sensor}-201[89]-*-*.csv\")\n",
    "    all_data = []\n",
    "    for file in csv_files:\n",
    "        # Read the CSV file\n",
    "        df = pd.read_csv(file, delimiter=';', parse_dates=[0], index_col=0)\n",
    "        \n",
    "        # Clean the column names of quotes\n",
    "        df.columns = [col.replace('\"', '').replace(\"'\", \"\") for col in df.columns]\n",
    "        df = df[~df.index.astype(str).str.contains('totaal')]\n",
    "    \n",
    "        df.index.name = None\n",
    "        df.index = pd.to_datetime(df.index)\n",
    "        # print(df.head())\n",
    "        \n",
    "        # Keep only the filtered columns\n",
    "        df = df.loc[:, codes]\n",
    "        # print(df.head())\n",
    "        \n",
    "        # Append the DataFrame to the list\n",
    "        all_data.append(df)\n",
    "\n",
    "    # Concatenate all DataFrames into a single DataFrame\n",
    "    final_df = pd.concat(all_data)\n",
    "\n",
    "    # Sort the DataFrame by the index (date and time)\n",
    "    final_df.sort_index(inplace=True)\n",
    "    final_df.columns = [sensor + \"-\" + col for col in final_df.columns]\n",
    "    final_df = final_df[final_df.index.year.isin([2018, 2019])]\n",
    "    \n",
    "\n",
    "    # Print the first few rows to verify\n",
    "    # print(final_df.head())\n",
    "    non_unique_indices = final_df.index.duplicated(keep=False)\n",
    "    non_unique_rows = final_df[non_unique_indices]\n",
    "\n",
    "    if not non_unique_rows.empty:\n",
    "        print(non_unique_rows)\n",
    "    intersections.append(final_df)\n",
    "# hague = pd.concat(intersections, axis=1)\n",
    "# hague = hague.sort_index(axis=1)\n",
    "\n",
    "# hague.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hague = pd.concat(intersections, axis=1)\n",
    "hague = hague.sort_index(axis=1)\n",
    "\n",
    "hague.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hague.to_csv(\"data/Hauge/hague.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try to impute the missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only select the columns with missing values < 34%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_percentage = hague.isna().mean() * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nan_percentage.sort_values(ascending=False)[:25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_below_34_nan = nan_percentage[nan_percentage < 34].index.tolist()\n",
    "\n",
    "# Selecting the columns in 'hague' DataFrame where the NaN percentage is less than 34%\n",
    "filtered_hague = hague[columns_below_34_nan]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imputation based on https://www.geo.fu-berlin.de/en/v/soga-py/Advanced-statistics/time-series-analysis/Dealing-with-missing-values/Imputing-missing-values/index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_hague.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_NA_inter = filtered_hague.interpolate(method=\"time\").copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_NA_inter.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_NA_bfill = temp_NA_inter.fillna(method=\"bfill\")\n",
    "temp_NA_bfill.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_NA_bfill.to_hdf('data/Hauge/hague_filled.h5', key='df', mode='w')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create new adjacency matrix Hague"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distances_df = pd.read_csv('data/Hauge/distances.csv')\n",
    "df = pd.read_hdf('data/Hauge/hague_filled.h5')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensor_ids = df.columns.astype(str).tolist()\n",
    "\n",
    "# Efficiently convert sensor IDs to index for quick lookup (now all as strings)\n",
    "sensor_id_to_index = {str(sensor_id): index for index, sensor_id in enumerate(sensor_ids)}\n",
    "\n",
    "# Initialize an empty distance matrix\n",
    "num_sensors = len(sensor_ids)\n",
    "dist_mx_array = np.full((num_sensors, num_sensors), np.inf)\n",
    "\n",
    "# Make sure 'from' and 'to' in distances_df are also strings\n",
    "distances_df['from'] = distances_df['from'].astype(str)\n",
    "distances_df['to'] = distances_df['to'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Populate the distance matrix with the data from distances_df\n",
    "for _, row in distances_df.iterrows():\n",
    "    from_sensor = row['from']\n",
    "    to_sensor = row['to']\n",
    "    if from_sensor in sensor_id_to_index and to_sensor in sensor_id_to_index:\n",
    "        i = sensor_id_to_index[from_sensor]\n",
    "        j = sensor_id_to_index[to_sensor]\n",
    "        dist_mx_array[i, j] = row['cost']\n",
    "\n",
    "# Replace the diagonal with zeros since the distance from a sensor to itself is zero\n",
    "np.fill_diagonal(dist_mx_array, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_adjacency_matrix(dist_mx_array, normalized_k=0.1):\n",
    "    # Calculate the standard deviation as theta for normalization\n",
    "    distances = dist_mx_array[~np.isinf(dist_mx_array)].flatten()\n",
    "    std = distances.std()\n",
    "    adj_mx_array = np.exp(-np.square(dist_mx_array / std))\n",
    "    \n",
    "    # Set entries below a threshold to zero for sparsity\n",
    "    adj_mx_array[adj_mx_array < normalized_k] = 0\n",
    "    return adj_mx_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_mx_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_mx_array = get_adjacency_matrix(dist_mx_array).astype(np.float32)\n",
    "\n",
    "# Construct the adj_mx list as specified with the updated adj_mx_array\n",
    "adj_mx = [sensor_ids, sensor_id_to_index, adj_mx_array]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_mx[2][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the file path\n",
    "directory = 'data/Hauge'\n",
    "file_name = 'adj_mx.pkl'\n",
    "file_path = os.path.join(directory, file_name)\n",
    "\n",
    "# Ensure the directory exists\n",
    "os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "# Save the adj_mx to a .pkl file\n",
    "with open(file_path, 'wb') as f:\n",
    "    pickle.dump(adj_mx, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a smaller size of the original data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_hdf('data/Hauge/hague_filled.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_rows = len(df)\n",
    "\n",
    "# Calculate indices for 12.5th and 87.5th percentiles\n",
    "start_index = int(total_rows * 0.125)\n",
    "end_index = int(total_rows * 0.875)\n",
    "\n",
    "# Select the middle 75% of the rows\n",
    "middle_75_df = df.iloc[start_index:end_index]\n",
    "middle_75_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "middle_75_df.to_hdf('data/Hauge/hague_filled_75.h5', key='df', mode='w')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# file_path = 'data/METRLA/adj_mx_new.pkl'\n",
    "file_path = 'data/Hauge/adj_mx.pkl'\n",
    "\n",
    "with open(file_path, 'rb') as f:\n",
    "    adj_mx = pickle.load(f, encoding='latin1')\n",
    "\n",
    "print(len(adj_mx))\n",
    "print(adj_mx[0])\n",
    "print(len(adj_mx[0]))\n",
    "print(adj_mx[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_mx[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_mx[2].T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform Hauge to a compressed sensor datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'data/Hauge/Hague_comp_sensor_ref.xlsx'\n",
    "df1 = pd.read_excel(file_path)\n",
    "df2 = pd.read_csv('data/Hauge/hague.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dataframes(df1, df2):\n",
    "    # New dataframe to hold results\n",
    "    df3 = pd.DataFrame()\n",
    "    \n",
    "    # Iterate over rows in df1\n",
    "    for idx, row in df1.iterrows():\n",
    "        intersection = row['Intersection']\n",
    "        original_columns = str(row['Original']).split(',')\n",
    "\n",
    "        # Create full column names in df2 and sum them\n",
    "        if len(original_columns) > 1:\n",
    "            new_column_name = intersection + '-' + row['New']\n",
    "            # Create a list of full names to search in df2\n",
    "            df2_column_names = [intersection + '-' + orig for orig in original_columns]\n",
    "            # Average the columns in df2 that are in our list if they exist\n",
    "            df3[new_column_name] = df2[df2_column_names].mean(axis=1)\n",
    "        else:\n",
    "            # Just copy the column from df2\n",
    "            df2_column_name = intersection + '-' + original_columns[0]\n",
    "            df3[df2_column_name] = df2[df2_column_name] if df2_column_name in df2.columns else pd.Series()\n",
    "    \n",
    "    return df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = process_dataframes(df1, df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.to_csv(\"data/Hauge/hague_comp.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_percentage = df3.isna().mean() * 100\n",
    "print(nan_percentage.sort_values(ascending=False)[:25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_below_34_nan = nan_percentage[nan_percentage < 30].index.tolist()\n",
    "\n",
    "# Selecting the columns in 'hague' DataFrame where the NaN percentage is less than 34%\n",
    "filtered_hague = df3[columns_below_34_nan]\n",
    "filtered_hague.index = pd.to_datetime(filtered_hague.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_NA_inter = filtered_hague.interpolate(method=\"time\").copy()\n",
    "temp_NA_bfill = temp_NA_inter.fillna(method=\"bfill\")\n",
    "temp_NA_bfill.to_hdf('data/Hauge/hague_comp_filled.h5', key='df', mode='w')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New adj based on combined sensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('data/HAUGE/distances.csv')\n",
    "df2 = pd.read_hdf('data/Hauge/hague_comp_filled.h5')\n",
    "df3 = pd.read_excel('data/Hauge/Hague_comp_sensor_ref.xlsx')\n",
    "df4 = pd.read_csv('data/Hauge/location_std.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_id(sensor_id, trajectory, df3):\n",
    "    # Split the sensor_id by '-'\n",
    "    parts = sensor_id.split('-')\n",
    "    if len(parts) != 2:\n",
    "        return None  # Return None if sensor_id does not match expected format\n",
    "    \n",
    "    # Find matching row in df3 with the same trajectory\n",
    "    mask = (df3['Intersection'] == parts[0]) & (df3['Representation'] == parts[1]) & (df3['Trajectory'] == trajectory)\n",
    "    if mask.any():\n",
    "        # If a matching row is found, replace with the 'New' value\n",
    "        return parts[0] + '-' + df3.loc[mask, 'New'].values[0]\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sensor_id</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>trajectory</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>K502-S1</td>\n",
       "      <td>52.051972</td>\n",
       "      <td>4.229278</td>\n",
       "      <td>T1N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>K502-051</td>\n",
       "      <td>52.051750</td>\n",
       "      <td>4.228972</td>\n",
       "      <td>T1N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>K502-111</td>\n",
       "      <td>52.052000</td>\n",
       "      <td>4.229806</td>\n",
       "      <td>T1N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>K504-N1</td>\n",
       "      <td>52.042250</td>\n",
       "      <td>4.240278</td>\n",
       "      <td>T1N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>K504-11_1</td>\n",
       "      <td>52.042500</td>\n",
       "      <td>4.239972</td>\n",
       "      <td>T1N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>K415-021</td>\n",
       "      <td>52.071609</td>\n",
       "      <td>4.341984</td>\n",
       "      <td>T2N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>K415-081</td>\n",
       "      <td>52.071553</td>\n",
       "      <td>4.341502</td>\n",
       "      <td>T2S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>K250-N1</td>\n",
       "      <td>52.067711</td>\n",
       "      <td>4.353475</td>\n",
       "      <td>T2N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>K250-N2</td>\n",
       "      <td>52.067414</td>\n",
       "      <td>4.353624</td>\n",
       "      <td>T2N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>K250-S1</td>\n",
       "      <td>52.067564</td>\n",
       "      <td>4.352998</td>\n",
       "      <td>T2S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>90 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     sensor_id   latitude  longitude trajectory\n",
       "1      K502-S1  52.051972   4.229278        T1N\n",
       "2     K502-051  52.051750   4.228972        T1N\n",
       "3     K502-111  52.052000   4.229806        T1N\n",
       "11     K504-N1  52.042250   4.240278        T1N\n",
       "14   K504-11_1  52.042500   4.239972        T1N\n",
       "..         ...        ...        ...        ...\n",
       "164   K415-021  52.071609   4.341984        T2N\n",
       "165   K415-081  52.071553   4.341502        T2S\n",
       "167    K250-N1  52.067711   4.353475        T2N\n",
       "169    K250-N2  52.067414   4.353624        T2N\n",
       "171    K250-S1  52.067564   4.352998        T2S\n",
       "\n",
       "[90 rows x 4 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def apply_transform_or_keep_original(row, df3):\n",
    "    trajectories = row['trajectory'].split('+')\n",
    "    new_ids = []\n",
    "\n",
    "    for trajectory in trajectories:\n",
    "        new_id = transform_id(row['sensor_id'], trajectory, df3)\n",
    "        if new_id is not None:\n",
    "            new_ids.append(new_id)\n",
    "\n",
    "    # If no new ID was found for either trajectory, keep the original sensor_id\n",
    "    if not new_ids:\n",
    "        new_ids.append(row['sensor_id'])\n",
    "\n",
    "    # Return a list of dictionaries for each new_id to expand into new rows\n",
    "    return [{'sensor_id': new_id, 'latitude': row['latitude'], 'longitude': row['longitude'], 'trajectory': trajectory} for new_id, trajectory in zip(new_ids, trajectories)]\n",
    "\n",
    "# Apply the transformation to generate new rows\n",
    "expanded_rows = df4.apply(lambda row: apply_transform_or_keep_original(row, df3), axis=1)\n",
    "\n",
    "# Flatten the list of lists into a single DataFrame\n",
    "df4_expanded = pd.DataFrame([item for sublist in expanded_rows for item in sublist])\n",
    "\n",
    "# Keep only the sensor IDs that are present in the columns of df2\n",
    "df2_columns = set(df2.columns)\n",
    "df4_filtered = df4_expanded[df4_expanded['sensor_id'].isin(df2_columns)]\n",
    "df4_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4_filtered.to_csv('data/Hauge/location_std_comp.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['new_from'] = df1.apply(lambda row: transform_id(row['from'], row['trajectory'], df3), axis=1)\n",
    "df1['new_to'] = df1.apply(lambda row: transform_id(row['to'], row['trajectory'], df3), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2_columns = set(df2.columns)\n",
    "df4 = df1[(df1['new_from'].isin(df2_columns)) & (df1['new_to'].isin(df2_columns))]\n",
    "df4.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4['from'] = df4['new_from']\n",
    "df4['to'] = df4['new_to']\n",
    "df4 = df4.drop(['new_from', 'new_to'], axis=1)\n",
    "df4.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4.to_csv('data/Hauge/distances_comp.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate the adjacency matrix for the combined sensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distances_df = pd.read_csv('data/Hauge/distances_comp.csv')\n",
    "df = pd.read_hdf('data/Hauge/hague_comp_filled.h5')\n",
    "\n",
    "sensor_ids = df.columns.astype(str).tolist()\n",
    "\n",
    "# Efficiently convert sensor IDs to index for quick lookup (now all as strings)\n",
    "sensor_id_to_index = {str(sensor_id): index for index, sensor_id in enumerate(sensor_ids)}\n",
    "\n",
    "# Initialize an empty distance matrix\n",
    "num_sensors = len(sensor_ids)\n",
    "dist_mx_array = np.full((num_sensors, num_sensors), np.inf)\n",
    "\n",
    "# Make sure 'from' and 'to' in distances_df are also strings\n",
    "distances_df['from'] = distances_df['from'].astype(str)\n",
    "distances_df['to'] = distances_df['to'].astype(str)\n",
    "\n",
    "# Populate the distance matrix with the data from distances_df\n",
    "for _, row in distances_df.iterrows():\n",
    "    from_sensor = row['from']\n",
    "    to_sensor = row['to']\n",
    "    if from_sensor in sensor_id_to_index and to_sensor in sensor_id_to_index:\n",
    "        i = sensor_id_to_index[from_sensor]\n",
    "        j = sensor_id_to_index[to_sensor]\n",
    "        dist_mx_array[i, j] = row['cost']\n",
    "\n",
    "# Replace the diagonal with zeros since the distance from a sensor to itself is zero\n",
    "np.fill_diagonal(dist_mx_array, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_adjacency_matrix(dist_mx_array, normalized_k=0.1):\n",
    "    # Calculate the standard deviation as theta for normalization\n",
    "    distances = dist_mx_array[~np.isinf(dist_mx_array)].flatten()\n",
    "    std = distances.std()\n",
    "    adj_mx_array = np.exp(-np.square(dist_mx_array / std))\n",
    "    \n",
    "    # Set entries below a threshold to zero for sparsity\n",
    "    adj_mx_array[adj_mx_array < normalized_k] = 0\n",
    "    return adj_mx_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_mx_array = get_adjacency_matrix(dist_mx_array).astype(np.float32)\n",
    "\n",
    "# Construct the adj_mx list as specified with the updated adj_mx_array\n",
    "adj_mx = [sensor_ids, sensor_id_to_index, adj_mx_array]\n",
    "adj_mx[2][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the file path\n",
    "directory = 'data/Hauge'\n",
    "file_name = 'adj_mx_comp1.pkl'\n",
    "file_path = os.path.join(directory, file_name)\n",
    "\n",
    "# Ensure the directory exists\n",
    "os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "# Save the adj_mx to a .pkl file\n",
    "with open(file_path, 'wb') as f:\n",
    "    pickle.dump(adj_mx, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate the adjacency matrix for the combined sensors with direction applied to only count in foward direction that are connected by lanes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distances_df = pd.read_csv('data/Hauge/distances_comp_directed.csv')\n",
    "df = pd.read_hdf('data/Hauge/hague_comp_filled.h5')\n",
    "\n",
    "sensor_ids = df.columns.astype(str).tolist()\n",
    "\n",
    "# Efficiently convert sensor IDs to index for quick lookup (now all as strings)\n",
    "sensor_id_to_index = {str(sensor_id): index for index, sensor_id in enumerate(sensor_ids)}\n",
    "\n",
    "# Initialize an empty distance matrix\n",
    "num_sensors = len(sensor_ids)\n",
    "dist_mx_array = np.full((num_sensors, num_sensors), np.inf)\n",
    "\n",
    "# Make sure 'from' and 'to' in distances_df are also strings\n",
    "distances_df['from'] = distances_df['from'].astype(str)\n",
    "distances_df['to'] = distances_df['to'].astype(str)\n",
    "\n",
    "# Populate the distance matrix with the data from distances_df\n",
    "for _, row in distances_df.iterrows():\n",
    "    from_sensor = row['from']\n",
    "    to_sensor = row['to']\n",
    "    if from_sensor in sensor_id_to_index and to_sensor in sensor_id_to_index:\n",
    "        i = sensor_id_to_index[from_sensor]\n",
    "        j = sensor_id_to_index[to_sensor]\n",
    "        dist_mx_array[i, j] = row['cost']\n",
    "\n",
    "# Replace the diagonal with zeros since the distance from a sensor to itself is zero\n",
    "np.fill_diagonal(dist_mx_array, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_adjacency_matrix(dist_mx_array, normalized_k=0.1):\n",
    "    # Calculate the standard deviation as theta for normalization\n",
    "    distances = dist_mx_array[~np.isinf(dist_mx_array)].flatten()\n",
    "    std = distances.std()\n",
    "    adj_mx_array = np.exp(-np.square(dist_mx_array / std))\n",
    "    \n",
    "    # Set entries below a threshold to zero for sparsity\n",
    "    adj_mx_array[adj_mx_array < normalized_k] = 0\n",
    "    return adj_mx_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_mx_array = get_adjacency_matrix(dist_mx_array).astype(np.float32)\n",
    "\n",
    "# Construct the adj_mx list as specified with the updated adj_mx_array\n",
    "adj_mx = [sensor_ids, sensor_id_to_index, adj_mx_array]\n",
    "adj_mx[2][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the file path\n",
    "directory = 'data/Hauge'\n",
    "file_name = 'adj_mx_comp2.pkl'\n",
    "file_path = os.path.join(directory, file_name)\n",
    "\n",
    "# Ensure the directory exists\n",
    "os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "# Save the adj_mx to a .pkl file\n",
    "with open(file_path, 'wb') as f:\n",
    "    pickle.dump(adj_mx, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save a copy of adjacency matrix to csv file for inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "METR-LA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'data/METRLA/adj_mx.pkl'\n",
    "with open(file_path, 'rb') as f:\n",
    "    adj_mx = pickle.load(f, encoding='latin1')\n",
    "np.savetxt(\"data/METRLA/adj_mx.csv\", adj_mx[2], delimiter=\",\", fmt='%.4f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'data/METRLA/adj_mx_new1.pkl'\n",
    "with open(file_path, 'rb') as f:\n",
    "    adj_mx = pickle.load(f, encoding='latin1')\n",
    "np.savetxt(\"data/METRLA/adj_mx_new1.csv\", adj_mx[2], delimiter=\",\", fmt='%.4f')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PeMS-BAY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'data/PEMSBAY/adj_mx_bay.pkl'\n",
    "with open(file_path, 'rb') as f:\n",
    "    adj_mx = pickle.load(f, encoding='latin1')\n",
    "np.savetxt(\"data/PEMSBAY/adj_mx_bay.csv\", adj_mx[2], delimiter=\",\", fmt='%.4f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'data/PEMSBAY/adj_mx_new1.pkl'\n",
    "with open(file_path, 'rb') as f:\n",
    "    adj_mx = pickle.load(f, encoding='latin1')\n",
    "np.savetxt(\"data/PEMSBAY/adj_mx_new1.csv\", adj_mx[2], delimiter=\",\", fmt='%.4f')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hauge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'data/Hauge/adj_mx1.pkl'\n",
    "with open(file_path, 'rb') as f:\n",
    "    adj_mx = pickle.load(f, encoding='latin1')\n",
    "np.savetxt(\"data/Hauge/adj_mx1.csv\", adj_mx[2], delimiter=\",\", fmt='%.4f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'data/Hauge/adj_mx_comp1.pkl'\n",
    "with open(file_path, 'rb') as f:\n",
    "    adj_mx = pickle.load(f, encoding='latin1')\n",
    "np.savetxt(\"data/Hauge/adj_mx_comp1.csv\", adj_mx[2], delimiter=\",\", fmt='%.4f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'data/Hauge/adj_mx_comp2.pkl'\n",
    "with open(file_path, 'rb') as f:\n",
    "    adj_mx = pickle.load(f, encoding='latin1')\n",
    "np.savetxt(\"data/Hauge/adj_mx_comp2.csv\", adj_mx[2], delimiter=\",\", fmt='%.4f')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Small size dataset Hague for cost model (memory saver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_hdf('data/Hauge/hague_comp_filled.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Middle 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate indices for the middle 20%\n",
    "start_idx = int(len(df) * 0.40)\n",
    "end_idx = int(len(df) * 0.60)\n",
    "\n",
    "# Select the middle 20% chunk\n",
    "df_middle_20_percent = df.iloc[start_idx:end_idx]\n",
    "\n",
    "# Convert index to datetime\n",
    "df_middle_20_percent.index = pd.to_datetime(df_middle_20_percent.index, unit='ns')\n",
    "\n",
    "# Save to HDF5 file\n",
    "df_middle_20_percent.to_hdf('data/Hauge/hague_comp_filled_20_2.h5', key='df', mode='w')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate indices for the middle 20%\n",
    "start_idx = int(len(df) * 0.0)\n",
    "end_idx = int(len(df) * 0.2)\n",
    "\n",
    "# Select the middle 20% chunk\n",
    "df_first_20_percent = df.iloc[start_idx:end_idx]\n",
    "\n",
    "# Convert index to datetime\n",
    "df_first_20_percent.index = pd.to_datetime(df_first_20_percent.index, unit='ns')\n",
    "\n",
    "# Save to HDF5 file\n",
    "df_first_20_percent.to_hdf('data/Hauge/hague_comp_filled_20_3.h5', key='df', mode='w')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import folium\n",
    "\n",
    "# Load the CSV data\n",
    "csv_file_path = 'data/METRLA/graph_sensor_locations.csv'\n",
    "data = pd.read_csv(csv_file_path)\n",
    "\n",
    "# Create a folium map centered around the mean latitude and longitude\n",
    "center_lat = data['latitude'].mean()\n",
    "center_lon = data['longitude'].mean()\n",
    "m = folium.Map(location=[center_lat, center_lon], zoom_start=10, tiles='OpenStreetMap')\n",
    "\n",
    "# Add sensor locations to the map\n",
    "for idx, row in data.iterrows():\n",
    "    folium.CircleMarker(\n",
    "        location=[row['latitude'], row['longitude']],\n",
    "        radius=5,\n",
    "        color='red',\n",
    "        fill=True,\n",
    "        fill_color='red',\n",
    "        fill_opacity=0.7,\n",
    "        popup=f'Sensor ID: {row[\"sensor_id\"]}'\n",
    "    ).add_to(m)\n",
    "\n",
    "# Save the map to an HTML file\n",
    "m.save('sensor_locations_map_METRLA.html')\n",
    "\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import folium\n",
    "\n",
    "# Load the CSV data\n",
    "csv_file_path = 'data/PEMSBAY/graph_sensor_locations_bay.csv'\n",
    "data = pd.read_csv(csv_file_path)\n",
    "\n",
    "# Create a folium map centered around the mean latitude and longitude\n",
    "center_lat = data['latitude'].mean()\n",
    "center_lon = data['longitude'].mean()\n",
    "m = folium.Map(location=[center_lat, center_lon], zoom_start=10, tiles='OpenStreetMap')\n",
    "\n",
    "# Add sensor locations to the map\n",
    "for idx, row in data.iterrows():\n",
    "    folium.CircleMarker(\n",
    "        location=[row['latitude'], row['longitude']],\n",
    "        radius=5,\n",
    "        color='red',\n",
    "        fill=True,\n",
    "        fill_color='red',\n",
    "        fill_opacity=0.7,\n",
    "        popup=f'Sensor ID: {row[\"sensor_id\"]}'\n",
    "    ).add_to(m)\n",
    "\n",
    "# Save the map to an HTML file\n",
    "m.save('sensor_locations_map_bay.html')\n",
    "\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import folium\n",
    "\n",
    "# Load the CSV data\n",
    "csv_file_path = 'data/Hauge/location_std.csv'\n",
    "data = pd.read_csv(csv_file_path)\n",
    "\n",
    "# Extract the first trajectory if multiple are concatenated\n",
    "data['trajectory'] = data['trajectory'].apply(lambda x: x.split('+')[0])\n",
    "\n",
    "# Predefined colors for the trajectories\n",
    "color_mapping = {\n",
    "    'T1N': 'red',\n",
    "    'T1S': 'orange',\n",
    "    'T2N': 'blue',\n",
    "    'T2S': 'purple'\n",
    "}\n",
    "\n",
    "# Create a folium map centered around the mean latitude and longitude\n",
    "center_lat = data['latitude'].mean()\n",
    "center_lon = data['longitude'].mean()\n",
    "m = folium.Map(location=[center_lat, center_lon], zoom_start=10, tiles='OpenStreetMap')\n",
    "\n",
    "# Add sensor locations to the map with specific colors for each trajectory\n",
    "for idx, row in data.iterrows():\n",
    "    trajectory = row['trajectory']\n",
    "    color = color_mapping.get(trajectory, 'gray')  # Default to gray if trajectory is not in color_mapping\n",
    "    folium.CircleMarker(\n",
    "        location=[row['latitude'], row['longitude']],\n",
    "        radius=5,\n",
    "        color=color,\n",
    "        fill=True,\n",
    "        fill_color=color,\n",
    "        fill_opacity=0.7,\n",
    "        popup=f'Sensor ID: {row[\"sensor_id\"]}, Trajectory: {row[\"trajectory\"]}'\n",
    "    ).add_to(m)\n",
    "\n",
    "# Save the map to an HTML file\n",
    "m.save('sensor_locations_map_Hauge.html')\n",
    "\n",
    "m"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
